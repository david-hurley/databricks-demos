{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "879d57d1-a122-45f4-b047-c4b42ba60d7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install dbl-tempo==0.1.29\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6877cc38-6f07-4c03-be8a-d4836d42407f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, date_sub, lit, rand\n",
    "from tempo import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fef36ae8-5217-4f20-a7b4-2639affb0977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Unity Catalog Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d34c6d6-25a7-43c9-88f5-68a1c93a0abd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"users\"\n",
    "schema = \"david_hurley\"\n",
    "volume = \"electricity_load_data\"\n",
    "filename = \"panama_electricity_load_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4a8034e-3847-47c4-b345-b6322da2adb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create Synthetic Bronze Electricity Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcd1ef40-4ad4-40f8-8733-51b733275fff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_load_data = spark.read.csv(f\"/Volumes/{catalog}/{schema}/{volume}/{filename}\", header=True, inferSchema=True)\n",
    "\n",
    "# keep certain columns and rename\n",
    "df_load_data = df_load_data.select(*['datetime', 'nat_demand'])\n",
    "df_load_data = df_load_data.toDF(*['datetime', 'load'])\n",
    "\n",
    "# keep only a subset of the data, based on date\n",
    "df_load_data = df_load_data.filter((col(\"datetime\") > \"2019-09-01\") & (col(\"datetime\") < \"2020-01-31\")).withColumn(\"tagId\", lit(\"1\"))\n",
    "\n",
    "# create fake tagId and fake load data for each tag\n",
    "synthetic_load_data = []\n",
    "for i in range(2, 10):\n",
    "  # fake load data is a multiple of load between 0.5 and 1.0\n",
    "  df_temp = df_load_data.withColumn(\"load\", col(\"load\") * (rand() * 0.5 + 0.5)).withColumn(\"tagId\", lit(str(i)))\n",
    "  synthetic_load_data.append(df_temp)\n",
    "\n",
    "# create a new combined dataset with fake load data for all tags\n",
    "df_load_data = df_load_data.unionByName(synthetic_load_data[0])\n",
    "for temp_df in synthetic_load_data[1:]:\n",
    "    df_load_data = df_load_data.unionByName(temp_df)\n",
    "\n",
    "# order by date and tagId and re-order columns\n",
    "df_synthetic_load_data = df_load_data.orderBy([\"datetime\", \"tagId\"], ascending=[True, True])\n",
    "df_synthetic_load_data = df_synthetic_load_data[[\"datetime\", \"tagId\", \"load\"]]\n",
    "\n",
    "# save to Bronze table\n",
    "df_synthetic_load_data.write.mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.bronze_electricity_load_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f1fdf9c-d6c8-4956-b919-6fcc2b552e43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create Synthetic Bronze Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddba09c7-e9ad-4e3f-b523-f0b2be05a145",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_load_data = spark.read.csv(f\"/Volumes/{catalog}/{schema}/{volume}/{filename}\", header=True, inferSchema=True)\n",
    "\n",
    "# keep certain columns and rename\n",
    "df_wx_data = df_load_data.select(*['datetime', 'T2M_toc'])\n",
    "df_wx_data = df_wx_data.toDF(*['datetime', 'temperature'])\n",
    "\n",
    "# keep only a subset of the data, based on date\n",
    "df_wx_data = df_wx_data.filter((col(\"datetime\") > \"2019-09-01\") & (col(\"datetime\") < \"2020-01-31\"))\n",
    "\n",
    "# resample to every minute and filter to keep only 10min interval\n",
    "# do this to demonstrate combining datasets of different datetime\n",
    "wx_tsdf = TSDF(df_wx_data, ts_col=\"datetime\", partition_cols=[])\n",
    "wx_tsdf_resampled = wx_tsdf.resample(freq='min', func='mean')\n",
    "wx_tsdf_interpolated = wx_tsdf_resampled.interpolate(method=\"linear\")\n",
    "\n",
    "wx_interpolated_df_pandas = wx_tsdf_interpolated.df.toPandas()\n",
    "wx_interpolated_df_pandas = wx_interpolated_df_pandas.sort_values(by=\"datetime\")\n",
    "wx_interpolated_df_pandas = wx_interpolated_df_pandas[wx_interpolated_df_pandas['datetime'].dt.minute % 10 == 0]\n",
    "\n",
    "# save to Bronze table\n",
    "spark.createDataFrame(wx_interpolated_df_pandas).write.mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.bronze_weather_data\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "kagglehub==0.3.12"
    ],
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2841115366943128,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "1. Create Synthetic Datasets",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
