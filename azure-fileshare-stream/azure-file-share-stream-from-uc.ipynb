{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7380622-5c3c-4b4e-80c0-870acbda408f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Using Azure Storage File Share client library for Python to write UC data to Azure File Share\n",
    "\n",
    "##### Steps\n",
    "1. Load data from UC table into Pandas dataframe\n",
    "2. Stream dataframe into cluster memory as csv string\n",
    "3. Setup connetion onto Azure File Share (storage account connection string, refer to image in repository)\n",
    "4. Upload in-memory csv string to file share\n",
    "\n",
    "##### Source\n",
    "[Azure File Share Python SDK](https://learn.microsoft.com/en-us/python/api/overview/azure/storage-file-share-readme?view=azure-python)\n",
    "\n",
    "![](connection-string.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc854f0f-c363-42ad-9816-1fa23aa25fb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from azure.storage.fileshare import ShareFileClient\n",
    "\n",
    "# in-memory text stream setup\n",
    "csv_buffer = io.StringIO()\n",
    "\n",
    "# read data from UC and convert to pandas dataframe\n",
    "df = spark.table(\"<catalog>.<schema>.<table>\").toPandas()\n",
    "\n",
    "# load csv string into memory, point string to start \n",
    "# Note - if this is a large dataframe you may need to increase job cluster RAM or\n",
    "# you may need to first chunk the dataframe, write each dataframe to a UC volume, then read \n",
    "# each smaller dataframe from the volume bcack to a pandas dataframe and then stream\n",
    "df.to_csv(csv_buffer, index=False)\n",
    "csv_buffer.seek(0)\n",
    "\n",
    "# azure storage account connection string - can copy from Azure portal directly\n",
    "connection_string=\"DefaultEndpointsProtocol=https;AccountName=<account_name>;AccountKey=<account_key>;EndpointSuffix=core.windows.net\"\n",
    "\n",
    "# create connection to file share and name of path of file to write\n",
    "file_client = ShareFileClient.from_connection_string(\n",
    "    conn_str=connection_string,\n",
    "    share_name=\"myshare\", # update this name\n",
    "    file_path=\"mydir/test.csv\" # update this path\n",
    ")\n",
    "\n",
    "# stream csv string from memory to file share\n",
    "file_client.upload_file(csv_buffer.getvalue())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "azure-file-share-stream-from-uc",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
