{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca6f8b83-1826-4246-b507-7408df61f7c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# `ai_parse_document()` Visualizer Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb069138-c247-45f1-8b7d-e54a7456ce7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Overview\n",
    "This notebook acts as the backend for the AI Parse Visualizer App. \n",
    "\n",
    "Documents are saved to the Volume `input` directory and parsed using `ai_parse_document()`. Results are saved in a Delta table. \n",
    "\n",
    "Any documents already in the results table will not be parsed again, unless `force_reprocess_all == True`. \n",
    "\n",
    "#### Prerequisites\n",
    "- You must have a existing catalog, schema, and volume  \n",
    "- You must create 2 directories named `input` and `output` in the volume\n",
    "- You must place a few documents in the `input` directory — it's suggested to keep the document length small to start with as `ai_parse_document()` takes time to process each page\n",
    "\n",
    "#### Instructions\n",
    "1. Update the variables in the next cell to reflect your catalog, schema, and volume name\n",
    "2. Update the `results_table` name, this is where parsed document results will be stored. You should avoid change the name of this table when re-running the notebook  \n",
    "3. If you want to parse a single file, you can enter the filename for `input_file`. Otherwise, leave blank and all documents not already in the results table will be parsed \n",
    "4. Run the notebook! \n",
    "\n",
    "#### Output\n",
    "\n",
    "You will end up with a catalog structure like below:\n",
    "\n",
    "```text\n",
    "catalog\n",
    "└─ schema\n",
    "   ├─ volume\n",
    "   │  ├─ input/\n",
    "   │  └─ output/\n",
    "   └─ tables\n",
    "      └─ parsed results table\n",
    "```\n",
    "\n",
    "#### Limitations\n",
    "- Removing documents from the volume `input` directory will not remove parsed results from the `output` directory or results table. Instead, you need to drop the documents from the results table to avoid surfacing in the AI Parse Visualizer App\n",
    "- This is meant to visualize parsed results on a test set. This is not meant for visualizing results over hundreds of documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b0c1117-9164-4755-9ea4-be1bd5480d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18c9b79c-bfae-4128-a917-aa3fe4a7caa1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "UPDATE THIS CELL"
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"\"\n",
    "schema = \"\"\n",
    "volume = \"\"\n",
    "results_table = \"\"\n",
    "input_file = \"\" # single file name (i.e. my.pdf) or leave blank\n",
    "\n",
    "# WARNING - setting to True will overwrite all previous parsed results in the ai_parse_results table\n",
    "force_reprocess_all = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5db6b50c-b973-4990-b8a2-ed4b13cc339c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DO NOT MODIFY BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67b4af82-e701-4f26-a546-3a2d37a1d818",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Results Table"
    }
   },
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS {catalog}.{schema}.{results_table} (\n",
    "    createdAt TIMESTAMP NOT NULL,\n",
    "    path STRING NOT NULL,\n",
    "    parsed VARIANT NOT NULL,\n",
    "    CONSTRAINT {results_table}_pk PRIMARY KEY (path)\n",
    "  )\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4f70841-7c2f-4c6b-af05-3ccf1d9bd9c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_unprocessed_filenames(volume_path: str, result_path: str):\n",
    "  \"\"\"Returns a list of filenames not existing in the results table\"\"\"\n",
    "\n",
    "  sql = f\"LIST 'dbfs:{volume_path}'\"\n",
    "  \n",
    "  input_volume_files = spark.sql(sql)\n",
    "    \n",
    "  sql = f\"SELECT path FROM {result_path}\"\n",
    "  processed_files = spark.sql(sql)\n",
    "    \n",
    "  unprocessed_files = input_volume_files.join(\n",
    "      processed_files,\n",
    "      on=\"path\",\n",
    "      how=\"left_anti\"\n",
    "  )\n",
    "  \n",
    "  unprocessed_filenames = [row.path.split(\"/\")[-1] for row in unprocessed_files.select(\"path\").collect()]\n",
    "  \n",
    "  if unprocessed_filenames:\n",
    "      process_glob = \"{{\" + \",\".join(unprocessed_filenames) + \"}}\"\n",
    "  else:\n",
    "      process_glob = None\n",
    "  \n",
    "  return process_glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb432ef-285a-4677-bb0a-fd245888d32a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "volume_path = f\"/Volumes/{catalog}/{schema}/{volume}\"\n",
    "results_table_path = f\"{catalog}.{schema}.{results_table}\"\n",
    "\n",
    "if force_reprocess_all:\n",
    "  files_to_process = \"*\"\n",
    "else:\n",
    "  files_to_process = get_unprocessed_filenames(f\"{volume_path}/input\", results_table_path)\n",
    "\n",
    "input_files_path = f\"{volume_path}/input/{files_to_process}\"\n",
    "output_results_path = f\"{volume_path}/output\"\n",
    "\n",
    "if files_to_process:\n",
    "  sql = f\"\"\"\n",
    "  WITH parsed_documents AS (\n",
    "      SELECT\n",
    "        path,\n",
    "        ai_parse_document(\n",
    "          content,\n",
    "          map(\n",
    "            'imageOutputPath', '{output_results_path}',\n",
    "            'descriptionElementTypes', '*'\n",
    "          )\n",
    "        ) AS parsed\n",
    "      FROM READ_FILES('{input_files_path}', format => 'binaryFile')\n",
    "    )\n",
    "  SELECT \n",
    "    current_timestamp() AS createdAt,\n",
    "    path,\n",
    "    parsed\n",
    "  FROM parsed_documents\n",
    "  \"\"\"\n",
    "\n",
    "  df = spark.sql(sql)\n",
    "\n",
    "  delta_table = DeltaTable.forName(spark, results_table_path)\n",
    "\n",
    "  delta_table.alias(\"target\").merge(\n",
    "      df.alias(\"source\"),\n",
    "      \"target.path = source.path AND source.path IS NOT NULL\"\n",
    "  ).whenMatchedUpdate(\n",
    "      set={\n",
    "          \"createdAt\": \"source.createdAt\",\n",
    "          \"path\": \"source.path\",\n",
    "          \"parsed\": \"source.parsed\"\n",
    "      }\n",
    "  ).whenNotMatchedInsert(\n",
    "      values={\n",
    "          \"path\": \"source.path\",\n",
    "          \"createdAt\": \"source.createdAt\",\n",
    "          \"parsed\": \"source.parsed\"\n",
    "      }\n",
    "  ).execute()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6278799014523785,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "ai_parse_visualizer_app_backend",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
